from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
import json
import os
import fitz

print("Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½ JSON, JSONL, PDF vÃ  TXT...")

data_dir = "../data/CleanJSON"

# JSON
def create_documents_from_json(data, source_file):
    docs = []
    for item in data:
        text = json.dumps(item, ensure_ascii=False, indent=2)
        metadata = {"source": source_file}
        docs.append(Document(page_content=text, metadata=metadata))
    return docs

# PDF
def create_documents_from_pdf(file_path):
    docs = []
    doc = fitz.open(file_path)
    for page_num, page in enumerate(doc, start=1):
        text = page.get_text()
        if text.strip():
            metadata = {"source": file_path, "page": page_num}
            docs.append(Document(page_content=text.strip(), metadata=metadata))
    return docs

# JSONL
def create_documents_from_jsonl(file_path):
    docs = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                try:
                    item = json.loads(line)
                    text = json.dumps(item, ensure_ascii=False, indent=2)
                    metadata = {"source": file_path}
                    docs.append(Document(page_content=text, metadata=metadata))
                except json.JSONDecodeError:
                    print(f"Lá»—i JSONL á»Ÿ dÃ²ng: {line}")
    return docs

# TXT
def create_documents_from_txt(file_path):
    docs = []
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
        if text.strip():
            metadata = {"source": file_path}
            docs.append(Document(page_content=text.strip(), metadata=metadata))
    return docs

# Chunk documents
def chunk_documents(documents, chunk_size=500, chunk_overlap=50):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    chunked_documents = splitter.split_documents(documents)
    
    # ÄÃ¡nh dáº¥u má»—i chunk báº±ng metadata
    for idx, chunk in enumerate(chunked_documents):
        chunk.metadata["chunk_index"] = idx + 1  # ÄÃ¡nh dáº¥u chunk thá»© máº¥y

    return chunked_documents

# Process all files
def process_files_in_directory(data_dir):
    for filename in os.listdir(data_dir):
        file_path = os.path.join(data_dir, filename)

        if filename.endswith(".json") and not filename.endswith(".jsonl"):
            print(f"ğŸŸ¡ JSON: {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            documents = create_documents_from_json(data, file_path)

        elif filename.endswith(".jsonl"):
            print(f"ğŸŸ£ JSONL: {file_path}")
            documents = create_documents_from_jsonl(file_path)

        elif filename.endswith(".pdf"):
            print(f"ğŸ”µ PDF: {file_path}")
            documents = create_documents_from_pdf(file_path)

        elif filename.endswith(".txt"):
            print(f"ğŸŸ¢ TXT: {file_path}")
            documents = create_documents_from_txt(file_path)

        else:
            continue

        # Chunk
        chunked_docs = chunk_documents(documents)

        # In ra káº¿t quáº£
        print(f"\n--- Káº¿t quáº£ trÆ°á»›c khi chunking ---")
        for doc in documents[:3]:  # In 3 document Ä‘áº§u tiÃªn tá»« má»—i loáº¡i file
            print(f"Source: {doc.metadata['source']}")
            print(f"Content: {doc.page_content[:500]}...")  # Chá»‰ in pháº§n Ä‘áº§u tiÃªn

        print(f"\n--- Káº¿t quáº£ sau khi chunking ---")
        for chunk in chunked_docs[:3]:  # In 3 chunk Ä‘áº§u tiÃªn tá»« káº¿t quáº£ chunking
            print(f"Chunk Index: {chunk.metadata['chunk_index']}")
            print(f"Chunk Content: {chunk.page_content[:500]}...")  # Chá»‰ in pháº§n Ä‘áº§u tiÃªn

        print(f"\n{'-'*50}\n")

# Cháº¡y
process_files_in_directory(data_dir)
